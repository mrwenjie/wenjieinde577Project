{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Ensemble Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble Methods for Diabetes Prediction\n",
    "\n",
    "Ensemble methods are machine learning techniques that combine multiple individual models to make more accurate and robust predictions. This notebook demonstrates the application of three ensemble methods: Voting Classifier, Bagging Classifier, and Random Forest Classifier, for predicting diabetes using the Pima Indians Diabetes Dataset.\n",
    "\n",
    "1. **Voting Classifier**: The Voting Classifier combines the predictions of multiple individual classifiers (Logistic Regression, Random Forest, and SVM) to make the final prediction. It works by taking the majority vote of the predictions made by each individual classifier. By combining the strengths of different classifiers, the Voting Classifier aims to improve the overall accuracy and robustness of the predictions.\n",
    "\n",
    "2. **Bagging Classifier**: The Bagging (Bootstrap Aggregating) Classifier creates multiple instances of a base estimator (in this case, a Decision Tree) on different subsets of the training data, obtained through bootstrap sampling. Each base estimator is trained independently on its respective subset, and their predictions are combined to make the final prediction. Bagging helps to reduce overfitting and improve the stability and accuracy of the predictions.\n",
    "\n",
    "3. **Random Forest Classifier**: The Random Forest Classifier is an extension of the Bagging method that introduces an additional layer of randomness. It creates an ensemble of Decision Trees, where each tree is trained on a random subset of features and a random subset of the training data. The final prediction is made by aggregating the predictions of all the individual trees. Random Forests handle high-dimensional data well, reduce overfitting, and provide feature importance scores.\n",
    "\n",
    "The notebook compares the performance of these ensemble methods with individual classifiers and analyzes their classification reports to assess their accuracy, precision, recall, and F1-score. By leveraging the power of ensemble methods, the notebook aims to improve the prediction of diabetes compared to using a single model.\n",
    "\n",
    "Additionally, the notebook demonstrates how to measure feature importance in a Random Forest Classifier, which helps identify the most influential features for diabetes prediction. This information can be valuable for understanding the underlying factors contributing to diabetes and guiding further analysis or interventions.\n",
    "\n",
    "Overall, the ensemble methods in this notebook showcase how combining multiple models can lead to more accurate and robust predictions in the context of diabetes prediction using the Pima Indians Diabetes Dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. We import the library that is needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we Load the dataset from the provided URL using pd.read_csv() and Separate the features (X) and the target variable (y) by dropping the 'Outcome' column from the feature set; and get the data ready to be training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "file_path = 'https://raw.githubusercontent.com/npradaschnor/Pima-Indians-Diabetes-Dataset/master/diabetes.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Separate features and target\n",
    "X = data.drop('Outcome', axis=1)\n",
    "y = data['Outcome']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.1 Voting Classifier\n",
    "Next, we are going to create the individual classifier and the voting classifier. We set the voting parameter to \"hard\", which means the majority vote among the individual classifiers is used for the final prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create individual classifiers\n",
    "log_clf = LogisticRegression(max_iter=1000) # set hemax_iter here to be larger so that it would converge.\n",
    "rnd_clf = RandomForestClassifier()\n",
    "svm_clf = SVC()\n",
    "\n",
    "# Create the voting classifier\n",
    "voting_clf = VotingClassifier(\n",
    "    estimators=[(\"lr\", log_clf), (\"rf\", rnd_clf), (\"svm\", svm_clf)],\n",
    "    voting=\"hard\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After that we train the individual classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM accuracy: 0.75\n",
      "Logistic Regression accuracy: 0.75\n",
      "Random Forest accuracy: 0.73\n"
     ]
    }
   ],
   "source": [
    "# Train and evaluate individual classifiers\n",
    "svm_clf.fit(X_train, y_train)\n",
    "svm_y_predict = svm_clf.predict(X_test)\n",
    "print(f\"SVM accuracy: {accuracy_score(y_test, svm_y_predict):.2f}\")\n",
    "\n",
    "log_clf.fit(X_train, y_train)\n",
    "log_y_predict = log_clf.predict(X_test)\n",
    "print(f\"Logistic Regression accuracy: {accuracy_score(y_test, log_y_predict):.2f}\")\n",
    "\n",
    "rnd_clf.fit(X_train, y_train)\n",
    "rnd_y_predict = rnd_clf.predict(X_test)\n",
    "print(f\"Random Forest accuracy: {accuracy_score(y_test, rnd_y_predict):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And last we train the voting classifier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Voting Classifier accuracy: 0.76\n"
     ]
    }
   ],
   "source": [
    "# Train and evaluate the voting classifier\n",
    "voting_clf.fit(X_train, y_train)\n",
    "y_predict = voting_clf.predict(X_test)\n",
    "print(f\"Voting Classifier accuracy: {accuracy_score(y_test, y_predict):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.2 Bagging Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is a Bagging Classifier?\n",
    "\n",
    "Bagging, short for Bootstrap Aggregating, is an ensemble learning technique used to improve the stability and accuracy of machine learning models. It works by creating multiple instances of a base estimator (e.g., decision trees, neural networks, etc.) on different subsets of the training data, and then combining their predictions to make the final prediction.\n",
    "\n",
    "The key steps involved in the bagging classifier are:\n",
    "\n",
    "1. **Bootstrap Sampling**: The original training dataset is sampled multiple times with replacement to create several bootstrap samples. Each bootstrap sample is used to train a separate instance of the base estimator.\n",
    "\n",
    "2. **Base Estimator Training**: For each bootstrap sample, a base estimator (e.g., a decision tree) is trained. The base estimators are trained independently on different bootstrap samples, which helps to reduce the variance of the individual models.\n",
    "\n",
    "3. **Aggregation**: After training all the base estimators, their predictions are combined to make the final prediction. In classification problems, the final prediction is typically made by taking the majority vote of the predictions from all the base estimators (this is known as \"hard voting\").\n",
    "\n",
    "The main advantage of the bagging classifier is that it helps to reduce the variance and overfitting of the base estimators, especially when the base estimators are unstable or sensitive to small changes in the training data (like decision trees). By combining the predictions from multiple diverse models, the bagging classifier can often achieve better generalization and higher accuracy compared to a single base estimator.\n",
    "\n",
    "It's important to note that the bagging classifier works best when the base estimators are unstable and have high variance but low bias. If the base estimators have high bias, other ensemble techniques like boosting may be more appropriate.\n",
    "\n",
    "In the provided code, the bagging classifier is created using the `BaggingClassifier` class from scikit-learn, and a decision tree is used as the base estimator. The number of estimators (decision trees) in the ensemble is set to 100, and the predictions from all these estimators are combined to make the final prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we create an instance of the DecisionTreeClassifier with max_depth=6 and random_state=42. This decision tree classifier will be used as the base estimator for the bagging classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a decision tree classifier\n",
    "tree_clf = DecisionTreeClassifier(max_depth=6, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part, we create an instance of the BaggingClassifier. We pass the tree_clf (the decision tree classifier) as the base_estimator, set n_estimators=100 (the number of estimators in the ensemble), and set random_state=42 for reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a bagging classifier\n",
    "bag_clf = BaggingClassifier(\n",
    "    base_estimator=tree_clf,\n",
    "    n_estimators=100,\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we train it and make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bagging Classifier Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.81      0.80       199\n",
      "           1       0.63      0.61      0.62       109\n",
      "\n",
      "    accuracy                           0.74       308\n",
      "   macro avg       0.71      0.71      0.71       308\n",
      "weighted avg       0.73      0.74      0.74       308\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train the bagging classifier\n",
    "bag_clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "bag_y_pred = bag_clf.predict(X_test)\n",
    "\n",
    "# Print the classification report\n",
    "print(\"Bagging Classifier Classification Report\")\n",
    "print(classification_report(y_test, bag_y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Classifier\n",
    "### What is a Random Forest Classifier?\n",
    "\n",
    "A Random Forest Classifier is another type of ensemble learning technique that combines multiple decision trees to improve the overall performance and robustness of the model. It is an extension of the bagging (Bootstrap Aggregating) method, with an additional layer of randomness introduced during the construction of each decision tree.\n",
    "\n",
    "The key steps involved in the Random Forest Classifier are:\n",
    "\n",
    "1. **Bootstrap Sampling**: Similar to bagging, the original training dataset is sampled multiple times with replacement to create several bootstrap samples.\n",
    "\n",
    "2. **Random Subspace Selection**: For each bootstrap sample, a random subset of features is selected to train the decision tree. This adds an extra layer of randomness and helps to reduce the correlation between the individual decision trees in the ensemble.\n",
    "\n",
    "3. **Decision Tree Training**: For each bootstrap sample and the randomly selected subset of features, a decision tree is trained. The trees are grown to their maximum depth without pruning.\n",
    "\n",
    "4. **Aggregation**: After training all the decision trees, their predictions are combined to make the final prediction. In classification problems, the final prediction is typically made by taking the majority vote of the predictions from all the decision trees (this is known as \"hard voting\").\n",
    "\n",
    "The Random Forest Classifier has several advantages over a single decision tree or a bagging ensemble:\n",
    "\n",
    "1. **Reduced Overfitting**: By introducing randomness in the feature selection process, the Random Forest Classifier reduces the risk of overfitting to the training data, leading to better generalization performance.\n",
    "\n",
    "2. **Improved Accuracy**: The ensemble nature of the Random Forest Classifier often results in higher accuracy compared to a single decision tree, as the combined predictions from multiple trees can correct for individual errors.\n",
    "\n",
    "3. **Robustness to Noise and Outliers**: The Random Forest Classifier is less sensitive to noise and outliers in the training data, as they are less likely to affect the entire ensemble.\n",
    "\n",
    "4. **Feature Importance Estimation**: Random Forests can provide an estimate of the importance of each feature in the dataset, which can be useful for feature selection and understanding the underlying data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the code for random forest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compare a decision tree, bagging classifier and the random forest classifier here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tree Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.72      0.74       199\n",
      "           1       0.53      0.59      0.56       109\n",
      "\n",
      "    accuracy                           0.67       308\n",
      "   macro avg       0.65      0.65      0.65       308\n",
      "weighted avg       0.68      0.67      0.68       308\n",
      " \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a decision tree classifier\n",
    "tree_clf = DecisionTreeClassifier(max_depth=15, random_state=42)\n",
    "tree_clf.fit(X_train, y_train)\n",
    "tree_y_pred = tree_clf.predict(X_test)\n",
    "print(\"Tree Classification Report\")\n",
    "print(classification_report(y_test, tree_y_pred), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bagging Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.86      0.80       199\n",
      "           1       0.66      0.48      0.55       109\n",
      "\n",
      "    accuracy                           0.73       308\n",
      "   macro avg       0.70      0.67      0.68       308\n",
      "weighted avg       0.72      0.73      0.72       308\n",
      " \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a bagging classifier\n",
    "bag_clf = BaggingClassifier(\n",
    "    base_estimator=DecisionTreeClassifier(max_depth=1, random_state=42),\n",
    "    n_estimators=500,\n",
    "    bootstrap=True,\n",
    "    n_jobs=-1\n",
    ")\n",
    "bag_clf.fit(X_train, y_train)\n",
    "bag_y_pred = bag_clf.predict(X_test)\n",
    "print(\"Bagging Classification Report\")\n",
    "print(classification_report(y_test, bag_y_pred), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.81      0.80       199\n",
      "           1       0.63      0.58      0.60       109\n",
      "\n",
      "    accuracy                           0.73       308\n",
      "   macro avg       0.70      0.70      0.70       308\n",
      "weighted avg       0.73      0.73      0.73       308\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a random forest classifier\n",
    "rf_clf = RandomForestClassifier(n_estimators=500, random_state=42)\n",
    "rf_clf.fit(X_train, y_train)\n",
    "rf_y_pred = rf_clf.predict(X_test)\n",
    "print(\"Random Forest Classification Report\")\n",
    "print(classification_report(y_test, rf_y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Report Analysis\n",
    "\n",
    "The classification report provides a detailed breakdown of the performance of a classification model across multiple metrics for each class. Let's analyze the reports for the three models: Decision Tree, Bagging Classifier, and Random Forest Classifier.\n",
    "\n",
    "**Accuracy**:\n",
    "- The Bagging Classifier and Random Forest Classifier have the same and higher accuracy (0.73) compared to the Decision Tree (0.67).\n",
    "\n",
    "**Precision for Positive Class (1)**:\n",
    "- Bagging Classifier: 0.66 (highest)\n",
    "- Random Forest Classifier: 0.63\n",
    "- Decision Tree: 0.53 (lowest)\n",
    "\n",
    "**Recall (Sensitivity) for Positive Class (1)**:\n",
    "- Decision Tree: 0.59 (highest)\n",
    "- Random Forest Classifier: 0.58\n",
    "- Bagging Classifier: 0.48 (lowest)\n",
    "\n",
    "**F1-score for Positive Class (1)**:\n",
    "- Random Forest Classifier: 0.60 (highest)\n",
    "- Decision Tree: 0.56\n",
    "- Bagging Classifier: 0.55 (lowest)\n",
    "\n",
    "Based on these metrics, the Random Forest Classifier appears to be the best-performing model overall, with the highest accuracy, a good balance between precision and recall (indicated by the highest F1-score for the positive class), and comparable performance to other models for the negative class.\n",
    "\n",
    "However, the choice of the best model depends on your specific requirements and the trade-offs between different metrics. If you prioritize precision over recall for the positive class, the Bagging Classifier might be a better choice. If you want to maximize recall for the positive class, the Decision Tree or Random Forest Classifier might be preferable.\n",
    "\n",
    "Additionally, you should consider factors such as the class imbalance in your dataset, the cost of misclassification errors, and the interpretability of the models when making the final decision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Feature Importance \n",
    "\n",
    "One cool thing about random forests is that these models make it simple to measure feature importance of each feature. Scikit-Learn does this by measuring a feature's importance by looking at how much the tree nodes that use that feature reduce impurity on average across all trees in the forest. For example, consider running the following code cell. \n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Glucose: 0.266\n",
      "BMI: 0.159\n",
      "Age: 0.142\n",
      "DiabetesPedigreeFunction: 0.119\n",
      "BloodPressure: 0.090\n",
      "Pregnancies: 0.086\n",
      "SkinThickness: 0.073\n",
      "Insulin: 0.065\n"
     ]
    }
   ],
   "source": [
    "# Get the feature names from the dataset\n",
    "feature_names = X.columns.tolist()\n",
    "\n",
    "# Get the feature importance scores from the random forest classifier\n",
    "feature_importances = rf_clf.feature_importances_\n",
    "\n",
    "# Create a list of (feature_name, feature_importance) tuples\n",
    "feature_importance_tuples = [(name, score) for name, score in zip(feature_names, feature_importances)]\n",
    "\n",
    "# Sort the list of tuples based on the feature importance scores in descending order\n",
    "sorted_feature_importances = sorted(feature_importance_tuples, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Print the sorted feature names and their importance scores\n",
    "for name, score in sorted_feature_importances:\n",
    "    print(f\"{name}: {score:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we could observe the most important feature to predict the diabetes is the Glucose."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mbapython",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
