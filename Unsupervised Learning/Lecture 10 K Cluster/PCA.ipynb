{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 10. Principal Component Analysis (PCA)\n",
    "\n",
    "## What is Principal Component Analysis (PCA)?\n",
    "\n",
    "Principal Component Analysis (PCA) is a dimensionality reduction technique used in machine learning and data analysis. It is a powerful tool for simplifying complex datasets by transforming the original set of features into a smaller set of uncorrelated variables called principal components.\n",
    "\n",
    "The main idea behind PCA is to find the directions (principal components) that maximize the variance in the data. These principal components are orthogonal (perpendicular) to each other, and they capture the most important patterns or characteristics in the data.\n",
    "\n",
    "### Why use PCA?\n",
    "\n",
    "PCA is useful in several scenarios, including:\n",
    "\n",
    "1. **Data Visualization**: PCA can be used to visualize high-dimensional data in 2D or 3D space by projecting the data onto the first few principal components.\n",
    "\n",
    "2. **Dimensionality Reduction**: PCA can reduce the number of features in a dataset by keeping only the most important principal components, which can improve model performance and computational efficiency.\n",
    "\n",
    "3. **Noise Reduction**: PCA can help remove noise and redundancy from the data by separating the signal (principal components) from the noise (remaining components).\n",
    "\n",
    "4. **Feature Extraction**: PCA can be used for feature extraction by transforming the original features into a new set of uncorrelated features (principal components).\n",
    "\n",
    "### How does PCA work?\n",
    "\n",
    "PCA follows these steps:\n",
    "\n",
    "1. **Standardize the data**: Center the data by subtracting the mean from each feature, and scale the data by dividing each feature by its standard deviation.\n",
    "\n",
    "2. **Calculate the covariance matrix**: Compute the covariance matrix that describes the variance and correlation between features.\n",
    "\n",
    "3. **Calculate the eigenvectors and eigenvalues**: Find the eigenvectors and eigenvalues of the covariance matrix. Eigenvectors represent the principal components, and eigenvalues represent the amount of variance captured by each principal component.\n",
    "\n",
    "4. **Select principal components**: Choose the top `k` eigenvectors (principal components) that capture the most variance in the data.\n",
    "\n",
    "5. **Project the data**: Transform the original data onto the new subspace defined by the selected principal components.\n",
    "\n",
    "After performing PCA, the transformed data (principal components) can be used as input for various machine learning algorithms or for data visualization and analysis.\n",
    "\n",
    "PCA is a powerful technique, but it also has limitations. It assumes that the data is linear and may not be suitable for non-linear data. Additionally, the principal components may not always have a clear interpretation, which can make the results difficult to understand.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the Pima Diabetes dataset again, we would like to demonstrate PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the dataset\n",
    "file_path = 'https://raw.githubusercontent.com/npradaschnor/Pima-Indians-Diabetes-Dataset/master/diabetes.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Split the data into features and target\n",
    "X = data.drop('Outcome', axis=1)\n",
    "y = data['Outcome']\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform PCA\n",
    "pca = PCA(n_components=2)  # Choose the number of principal components\n",
    "X_pca = pca.fit_transform(X_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we only choose two components, and see how effective it is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7077922077922078\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Split the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_pca, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a logistic regression model on the principal components\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = logreg.predict(X_test)\n",
    "\n",
    "# Calculate the accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems like the accuracy has decreased from 75% from previous notes (say ensemble) to 70%, that means in PCA there is still some informaiton loss."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mbapython",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
