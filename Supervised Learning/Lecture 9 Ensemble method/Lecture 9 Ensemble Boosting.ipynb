{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 9.2 Ensemble Boosting \n",
    "\n",
    "**Boosting** is an ensemble method that trains predictors sequentially, each trying to correct the errors made by the previous predictor. In this lecture we consider two well known boosting methods, namely **AdaBoost** and **gradient boosting**. \n",
    "\n",
    "### 9.2.1 Adaboost\n",
    "\n",
    "### What is AdaBoost?\n",
    "\n",
    "AdaBoost, short for **Adaptive Boosting**, is a powerful ensemble learning technique used for classification problems. It works by combining multiple weak learners (simple models that perform slightly better than random guessing) to create a strong learner that can make accurate predictions.\n",
    "\n",
    "The main idea behind AdaBoost is to focus on the samples that are difficult to classify correctly by the previous weak learners. It does this by assigning higher weights to the misclassified samples, making the next weak learner pay more attention to these samples during training.\n",
    "\n",
    "### How Does AdaBoost Work?\n",
    "\n",
    "AdaBoost works in an iterative manner, where weak learners are added to the ensemble one by one. Here's a step-by-step explanation of the process:\n",
    "\n",
    "1. **Initialize weights**: Initially, all training samples are assigned equal weights.\n",
    "2. **Train a weak learner**: A weak learner (e.g., a decision tree stump) is trained on the weighted training data.\n",
    "3. **Calculate error rate**: The error rate of the weak learner is calculated based on the misclassified samples and their weights.\n",
    "4. **Adjust weights**: The weights of the misclassified samples are increased, and the weights of the correctly classified samples are decreased. This way, the next weak learner will focus more on the difficult samples.\n",
    "5. **Add weak learner to ensemble**: The weak learner is added to the ensemble, and its contribution is determined by its error rate. Weak learners with lower error rates contribute more to the final prediction.\n",
    "6. **Repeat**: Steps 2-5 are repeated until a specified number of weak learners are added or the desired performance is achieved.\n",
    "\n",
    "### Final Prediction\n",
    "\n",
    "Once all the weak learners have been trained and added to the ensemble, the final prediction is made by taking a weighted vote of all the weak learners. Each weak learner's prediction is weighted by its contribution (determined by its error rate), and the class with the highest weighted vote is chosen as the final prediction.\n",
    "\n",
    "### Purpose of AdaBoost\n",
    "\n",
    "AdaBoost is a powerful technique for several reasons:\n",
    "\n",
    "1. **Improved Accuracy**: By combining multiple weak learners, AdaBoost can achieve higher accuracy than any single weak learner alone.\n",
    "2. **Robustness**: AdaBoost is relatively robust to overfitting, as the ensemble combines multiple models, reducing the impact of individual model errors.\n",
    "3. **Feature Selection**: AdaBoost can be used for feature selection by analyzing the importance of features in the ensemble model.\n",
    "4. **Flexibility**: AdaBoost can work with various types of weak learners, such as decision trees, neural networks, or even simple rules.\n",
    "\n",
    "AdaBoost has been successfully applied in many real-world applications, such as object detection, face recognition, spam filtering, and medical diagnosis.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use our diabete dataset as an example. First Load our diabete dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Load the dataset\n",
    "file_path = 'https://raw.githubusercontent.com/npradaschnor/Pima-Indians-Diabetes-Dataset/master/diabetes.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Split the data into features (X) and target (y)\n",
    "X = data.drop('Outcome', axis=1)\n",
    "y = data['Outcome']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoost Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.87      0.83        99\n",
      "           1       0.72      0.60      0.65        55\n",
      "\n",
      "    accuracy                           0.77       154\n",
      "   macro avg       0.76      0.73      0.74       154\n",
      "weighted avg       0.77      0.77      0.77       154\n",
      " \n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create an AdaBoost Classifier\n",
    "ada_clf = AdaBoostClassifier(\n",
    "    base_estimator=DecisionTreeClassifier(max_depth=1, random_state=42),\n",
    "    n_estimators=10,\n",
    "    algorithm=\"SAMME.R\",\n",
    "    learning_rate=0.5\n",
    ")\n",
    "\n",
    "# Train the AdaBoost Classifier\n",
    "ada_clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "ada_y_pred = ada_clf.predict(X_test)\n",
    "\n",
    "# Print the classification report\n",
    "print(\"AdaBoost Classification Report\")\n",
    "print(classification_report(y_test, ada_y_pred), \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9.2.2 Gradient Boost:\n",
    "\n",
    "\n",
    "### What is Gradient Boosting?\n",
    "\n",
    "Gradient Boosting is another powerful ensemble learning technique used for regression and classification problems. Like AdaBoost, it combines multiple weak learners (usually decision trees) to create a strong predictive model. However, the way it builds the ensemble is different from AdaBoost.\n",
    "\n",
    "The main idea behind Gradient Boosting is to sequentially train weak learners on the residual errors of the previous learners, gradually improving the model's performance.\n",
    "\n",
    "### How Does Gradient Boosting Work?\n",
    "\n",
    "Gradient Boosting works in an iterative manner, where weak learners are added to the ensemble one by one. Here's a step-by-step explanation of the process:\n",
    "\n",
    "1. **Initialize model**: Start with a simple model (e.g., a decision tree stump or a constant value) as the initial prediction.\n",
    "2. **Calculate residuals**: Compute the residuals (errors) between the current prediction and the actual target values.\n",
    "3. **Fit a weak learner**: Train a weak learner (e.g., a decision tree) on the residuals, not the original data.\n",
    "4. **Update the model**: Update the ensemble model by adding the new weak learner with a small learning rate (shrinkage factor) to avoid overfitting.\n",
    "5. **Repeat**: Steps 2-4 are repeated until a specified number of weak learners are added or the desired performance is achieved.\n",
    "\n",
    "### Final Prediction\n",
    "\n",
    "Once all the weak learners have been trained and added to the ensemble, the final prediction is made by summing the predictions of all the weak learners in the ensemble.\n",
    "\n",
    "### Purpose of Gradient Boosting\n",
    "\n",
    "Gradient Boosting is a powerful technique for several reasons:\n",
    "\n",
    "1. **Improved Accuracy**: By combining multiple weak learners, Gradient Boosting can achieve higher accuracy than any single weak learner alone.\n",
    "2. **Robustness**: Gradient Boosting is relatively robust to outliers and noisy data, as it focuses on the residuals rather than the original data.\n",
    "3. **Feature Selection**: Gradient Boosting can be used for feature selection by analyzing the importance of features in the ensemble model.\n",
    "4. **Flexibility**: Gradient Boosting can work with various types of weak learners, such as decision trees, neural networks, or even simple rules.\n",
    "\n",
    "Gradient Boosting has been successfully applied in many real-world applications, such as ranking systems, recommendation engines, and predictive modeling.\n",
    "\n",
    "### Differences from AdaBoost\n",
    "\n",
    "While both AdaBoost and Gradient Boosting are ensemble techniques, they differ in their approach:\n",
    "\n",
    "- AdaBoost focuses on misclassified samples and adjusts their weights, while Gradient Boosting focuses on the residual errors.\n",
    "- AdaBoost combines weak learners using a weighted voting mechanism, while Gradient Boosting sums the predictions of the weak learners.\n",
    "- Gradient Boosting is more resistant to overfitting due to the use of a learning rate (shrinkage factor), which limits the contribution of each weak learner.\n",
    "\n",
    "In general, Gradient Boosting is considered more powerful and flexible than AdaBoost, but it can be more prone to overfitting if not configured properly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we trained a gradiant boost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Boosting Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.77      0.78        99\n",
      "           1       0.61      0.65      0.63        55\n",
      "\n",
      "    accuracy                           0.73       154\n",
      "   macro avg       0.71      0.71      0.71       154\n",
      "weighted avg       0.73      0.73      0.73       154\n",
      " \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# Create a Gradient Boosting Classifier\n",
    "gb_clf = GradientBoostingClassifier(\n",
    "    max_depth=2,\n",
    "    n_estimators=150,\n",
    "    learning_rate=0.2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Train the Gradient Boosting Classifier\n",
    "gb_clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "gb_y_pred = gb_clf.predict(X_test)\n",
    "\n",
    "# Print the classification report\n",
    "print(\"Gradient Boosting Classification Report\")\n",
    "print(classification_report(y_test, gb_y_pred), \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mbapython",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
